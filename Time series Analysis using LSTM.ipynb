{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis using lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our dataset to be able to evaluate our models\n",
    "\n",
    "# creating dict for result\n",
    "resultsDict = {}\n",
    "predictionsDict = {}\n",
    "\n",
    "air_pollution = pd.read_csv('air_pollution.csv', parse_dates=['date'])\n",
    "air_pollution.set_index('date', inplace=True)\n",
    " \n",
    "split_date = '2014-01-01'\n",
    "df_training = air_pollution.loc[air_pollution.index <= split_date]\n",
    "df_test = air_pollution.loc[air_pollution.index > split_date]\n",
    "print(f\"{len(df_training)} days of training data \\n {len(df_test)} days of testing data \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD time features to our model\n",
    "def create_time_features(df, target=None):\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['sin_day'] = np.sin(df['dayofyear'])\n",
    "    df['cos_day'] = np.cos(df['dayofyear'])\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    X = df.drop(['date'], axis=1)\n",
    "    if target:\n",
    "        y = df[target]\n",
    "        X = X.drop([target], axis=1)\n",
    "        return X, y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, y_train = create_time_features(\n",
    "df_training, target='pollution_today')\n",
    "X_test_df, y_test = create_time_features(df_test, target='pollution_today')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_df)  # No cheating, never scale on the training+test!\n",
    "X_train = scaler.transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=X_train_df.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our dl model we will create windows of data that will be feeded into the datasets, for each timestemp T we will append the data from T-7 to T to the Xdata with target Y(t)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100\n",
    "WINDOW_LENGTH = 24\n",
    "\n",
    "\n",
    "def window_data(X, Y, window=7):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(window-1, len(X)):\n",
    "        x.append(X[i-window+1:i+1])\n",
    "        y.append(Y[i])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "# Since we are doing sliding, we need to join the datasets again of train and test\n",
    "X_w = np.concatenate((X_train, X_test))\n",
    "y_w = np.concatenate((y_train, y_test))\n",
    "\n",
    "X_w, y_w = window_data(X_w, y_w, window=WINDOW_LENGTH)\n",
    "X_train_w = X_w[:-len(X_test)]\n",
    "y_train_w = y_w[:-len(X_test)]\n",
    "X_test_w = X_w[-len(X_test):]\n",
    "y_test_w = y_w[-len(X_test):]\n",
    "\n",
    "# Check we will have same test set as in the previous models, make sure we didnt screw up on the windowing\n",
    "print(f\"Test set equal: {np.array_equal(y_test_w,y_test)}\")\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_w, y_train_w))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_test_w, y_test_w))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        128, input_shape=X_train_w.shape[-2:], dropout=dropout),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='rmsprop', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 5\n",
    "\n",
    "model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS,\n",
    "                                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                      validation_data=val_data, validation_steps=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = simple_lstm_model.predict(X_test_w).reshape(1, -1)[0]\n",
    "resultsDict['Tensorflow simple LSTM'] = evaluate(y_test, yhat)\n",
    "predictionsDict['Tensorflow simple LSTM'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_dict['xgboost'] = \n",
    "\n",
    "df.append('gboost', y_pred, r2_score, mape, mse, mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
